{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffd6ec7",
   "metadata": {},
   "source": [
    "## 13. **Convolutional Neural Network (CNN)**\n",
    "\n",
    "- **Convolution 연산**: kernel을 사용하여 이미지의 특징(feature)을 추출\n",
    "- kernel은 이미지의 일부분과 연산을 통해 feature map을 생성\n",
    "- padding: 이미지 외곽의 정보 손실 방지를 위해 가장자리에 0을 추가해 출력 크기를 입력 크기와 동일하게 유지하고 싶을 때 사용\n",
    "- stride: kernel이 한 번에 이동하는 간격으로 stride가 크면 feature map 크기가 작아짐\n",
    "- dilation: kernel의 사이를 늘려서 더 넓은 범위의 정보를 반영하는 방식\n",
    "    \n",
    "    $$\n",
    "    H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2P - K + (K - 1) \\cdot (D - 1)}{S} \\right\\rfloor + 1\n",
    "    $$\n",
    "    \n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2P - K + (K - 1) \\cdot (D - 1)}{S} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "- 특징 추출(feature extraction) → 활성화 함수 → Pooling → 반복\n",
    "\n",
    "## 14. **CNN Depth Channel**\n",
    "\n",
    "- kernel을 여러 개(n개) 사용하면 n개의 feature map을 얻을 수 있음\n",
    "- CNN 아키텍처 흐름:\n",
    "    - 입력 이미지: `3 × H × W`\n",
    "    - kernel: `3 × K × K` (K는 커널 크기) x n개\n",
    "    - Convolution 연산 → `n × H' × W'` feature maps 생성\n",
    "    - 활성화 함수 적용 후 여러 층 반복(feature extraction)\n",
    "    - 마지막에 Flatten → Fully Connected Layer로 분류(Classfication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3586b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CPU/GPU 선택\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f61459c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 이미지 변환(전처리)\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# CIFAR-10은 10개의 클래스에 걸쳐 총 60,000개의 32x32 컬러 이미지로 구성된 데이터셋\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3336f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # 1. feature_extractor\n",
    "    self.feature_extractor = nn.Sequential(\n",
    "        # 3 input channels, 16 output channels, 5x5 kernel\n",
    "        # 커널 차원은 [16, 3, 5, 5]\n",
    "        nn.Conv2d(3, 16, 5, padding=2), # 16 * 28 * 28\n",
    "        nn.ReLU(),\n",
    "        # 16 input channels, 32 output channels, 5x5 kernel\n",
    "        # 커널 차원은 [32, 16, 5, 5]\n",
    "        nn.Conv2d(16, 32, 5, padding=2), # 32 * 24 * 24\n",
    "        nn.ReLU(),\n",
    "        # 32 input channels, 64 output channels, 5x5 kernel\n",
    "        # 커널 차원은 [64, 32, 5, 5]\n",
    "        nn.Conv2d(32, 64, 5, padding=2), # 64 * 20 * 20\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    # 2. classifier \n",
    "    self.classfier = nn.Sequential(\n",
    "      nn.Linear(64 * 20 * 20, 120),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(120, 84),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(84, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.feature_extractor(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.classfier(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cc128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.5602, Val Loss: 0.3663, Accuracy: 47.06%\n",
      "Epoch 2, Loss: 2.1491, Val Loss: 0.3474, Accuracy: 51.55%\n",
      "Epoch 3, Loss: 1.1361, Val Loss: 0.3207, Accuracy: 55.89%\n",
      "Epoch 4, Loss: 0.7304, Val Loss: 0.3422, Accuracy: 55.99%\n",
      "Epoch 5, Loss: 1.1589, Val Loss: 0.3321, Accuracy: 57.10%\n",
      "Epoch 6, Loss: 0.2928, Val Loss: 0.3786, Accuracy: 55.46%\n",
      "Epoch 7, Loss: 0.3917, Val Loss: 0.3768, Accuracy: 56.60%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = SimpleCNN()\n",
    "net.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "  net.train()\n",
    "  for batch_idx, (data, label) in enumerate(train_loader):\n",
    "    data, label = data.to(device), label.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = net(data)\n",
    "    train_loss = loss_fn(output, label)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  net.eval()\n",
    "  val_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, label in val_loader:\n",
    "      data, label = data.to(device), label.to(device)\n",
    "      output = net(data)\n",
    "      val_loss += loss_fn(output, label).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "  \n",
    "  val_loss /= len(val_loader.dataset)\n",
    "  accuracy = 100. * correct / len(val_loader.dataset)\n",
    "\n",
    "  print(f'Epoch {epoch+1}, Loss: {train_loss.item():.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
