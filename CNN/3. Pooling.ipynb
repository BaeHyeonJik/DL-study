{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00769a74",
   "metadata": {},
   "source": [
    "## 15. **Pooling layer**\n",
    "\n",
    "- 텐서의 특정 구역에서 특정 값(max, average)으로 대표값을 추출해 **차원을 줄이는 역할**을 함\n",
    "- 연산량 감소, 과적합 방지, 중요한 정보 유지에 효과적\n",
    "- Modern CNN\n",
    "    - Pooling 대신 **stride가 있는 Convolution**을 사용함\n",
    "        \n",
    "        ⇒ Pooling은 아키텍처 상 **정보 손실**이 있고 stride를 통해서도 차원을 줄일 수 있음\n",
    "        \n",
    "    - **GAP (Global Average Pooling)**: Channel별로 전체 값을 평균내어 1개의 값으로 축소\n",
    "        \n",
    "        ⇒ 이미지의 위치에 관계없이 좋은 성능\n",
    "        \n",
    "        ⇒ 차원 축소 + **Flatten + Fully Connected Layer 생략 가능**\n",
    "        \n",
    "        ⇒ 바로 출력층과 연결 → 연산량 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a45af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CPU/GPU 선택\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20550b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor shape: torch.Size([1, 1, 4, 4])\n",
      "Output Tensor shape: torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_tensor = torch.Tensor([[[[1, 2, 3, 4],\n",
    "                             [5, 6, 7, 8],\n",
    "                             [9, 10, 11, 12],\n",
    "                             [13, 14, 15, 16],\n",
    "                             ]]])\n",
    "\n",
    "# Pooling layer 생성, 절반으로 줄이기\n",
    "maxpool_layer = nn.MaxPool2d(kernel_size=2)\n",
    "output_tensor = maxpool_layer(input_tensor)\n",
    "print(f'Input Tensor shape: {input_tensor.shape}')\n",
    "print(f'Output Tensor shape: {output_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4d4f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 이미지 변환(전처리)\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# CIFAR-10은 10개의 클래스에 걸쳐 총 60,000개의 32x32 컬러 이미지로 구성된 데이터셋\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2800dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ModernGAPCNN(nn.Module):\n",
    "  def __init__(self, num_classes=10):\n",
    "    super().__init__()\n",
    "\n",
    "    self.feature_extractor = nn.Sequential(\n",
    "      # 첫번째는 최대한 많은 정보를 담기 위해 stride=1로 설정\n",
    "      nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=1), # 16 × 30 × 30\n",
    "      nn.ReLU(inplace=True),\n",
    "      # stride를 2로 설정하여 feature map 크기를 줄임(pooling 대체)\n",
    "      nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=1), # 32 × 15 × 15\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=1), # 64 × 7 × 7\n",
    "      nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "    # Global Average Pooling을 이용해 Channel별로 전체 값을 평균내어 1개의 값으로 축소\n",
    "    self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # 64 × 1 × 1\n",
    "    self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.feature_extractor(x)\n",
    "      x = self.global_avg_pool(x)\n",
    "      x = torch.flatten(x, 1)\n",
    "      x = self.classifier(x)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1187888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1462, Val Loss: 0.3513, Accuracy: 49.58%\n",
      "Epoch 2, Loss: 1.3162, Val Loss: 0.3160, Accuracy: 54.68%\n",
      "Epoch 3, Loss: 1.9586, Val Loss: 0.2873, Accuracy: 60.09%\n",
      "Epoch 4, Loss: 0.4086, Val Loss: 0.2747, Accuracy: 60.99%\n",
      "Epoch 5, Loss: 0.2992, Val Loss: 0.2584, Accuracy: 63.62%\n",
      "Epoch 6, Loss: 0.4884, Val Loss: 0.2623, Accuracy: 63.80%\n",
      "Epoch 7, Loss: 0.7368, Val Loss: 0.2522, Accuracy: 65.15%\n",
      "Epoch 8, Loss: 0.2222, Val Loss: 0.2531, Accuracy: 65.41%\n",
      "Epoch 9, Loss: 0.3516, Val Loss: 0.2534, Accuracy: 65.73%\n",
      "Epoch 10, Loss: 0.5158, Val Loss: 0.2457, Accuracy: 66.41%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = ModernGAPCNN(10)\n",
    "net.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "  net.train()\n",
    "  for batch_idx, (data, label) in enumerate(train_loader):\n",
    "    data, label = data.to(device), label.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = net(data)\n",
    "    train_loss = loss_fn(output, label)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  net.eval()\n",
    "  val_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "      data, label = data.to(device), label.to(device)\n",
    "      output = net(data)\n",
    "      val_loss += loss_fn(output, label).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "  \n",
    "  val_loss /= len(test_loader.dataset)\n",
    "  accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "  print(f'Epoch {epoch+1}, Loss: {train_loss.item():.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
