{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee17d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# CPU/GPU 선택\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff45e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens_id Shape: torch.Size([1258315])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "with open(\"data/bible.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    tokenizer = tiktoken.encoding_for_model('gpt2')\n",
    "    tokens_id = tokenizer.encode(text)\n",
    "tokens_id = torch.tensor(tokens_id, dtype=torch.long)\n",
    "print(f'Tokens_id Shape: {tokens_id.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de1be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283113\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, token_ids, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    # 슬라이딩 윈도우 방식으로 input과 target 시퀀스를 생성\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i: i + max_length]\n",
    "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(input_chunk)\n",
    "      self.target_ids.append(target_chunk)\n",
    "\n",
    "  # 전체 샘플 수를 반환 (DataLoader에서 사용)\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  \n",
    "  # 인덱스에 해당하는 input, target 시퀀스 반환\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n",
    "  \n",
    "\n",
    "def get_loaders(token_ids: list[int]) -> DataLoader:\n",
    "\n",
    "  # MyDataset 클래스에 token_ids를 전달하여 데이터셋을 만듦\n",
    "  dataset = MyDataset(token_ids, max_length = 32, stride = 4)\n",
    "\n",
    "  total_size = len(dataset)\n",
    "  train_size = int(total_size * 0.9)\n",
    "  val_size = total_size - train_size\n",
    "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "  # DataLoader 객체를 생성(train_loader, val_loader)\n",
    "  # dataset: 데이터를 배치 단위로 반환할 MyDataset 객체\n",
    "  # batch_size: 배치 크기\n",
    "  # shuffle: 데이터를 섞어서 반환\n",
    "  # drop_last: 마지막 배치가 배치 크기보다 작으면 버림\n",
    "  train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "  return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = get_loaders(tokens_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa218533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "EMB_DIM = 768\n",
    "CONTEXT_LENGTH = 128\n",
    "NUM_HEADS = 12\n",
    "NUM_LAYERS = 12\n",
    "DROP_RATE = 0.1\n",
    "\n",
    "# MultiHeadAttention class 정의\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, embed_dim, atten_dim, drop_rate, context_length):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_out = atten_dim\n",
    "\n",
    "    # Query, Key, Value 선형 레이어 정의\n",
    "    # W_query: 내가 어떤 정보를 찾고 싶은지를 표현 (질문 역할)\n",
    "    # W_key  : 각 토큰이 어떤 정보인지 표현 (정보의 제목)\n",
    "    # W_value: 실제로 전달할 정보 (실제 내용)\n",
    "    self.W_query = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "    self.W_key = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "    self.W_value = nn.Linear(embed_dim, atten_dim, bias=False)\n",
    "\n",
    "    # 드롭아웃 정의\n",
    "    self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    # 마스크 등록 (상삼각 행렬을 사용 => 정답지 차단)\n",
    "    mask = torch.triu(torch.ones((context_length, context_length)), diagonal=1)\n",
    "    self.register_buffer('mask', mask)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Query, Key, Value 생성\n",
    "    b, num_tokens, _ = x.shape\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    # 어텐션 스코어 계산 (QKᵀ)\n",
    "    attn_scores = queries @ keys.transpose(-2, -1)\n",
    "\n",
    "    # 마스크 적용(미래 시점의 토근에는 attention 방지)\n",
    "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "    # 스케일 조정 후 softmax로 어텐션 가중치 계산(atten_dim 기준), 이후 드롭아웃 적용\n",
    "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    # 컨텍스트 벡터 계산\n",
    "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "    context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "\n",
    "    return context_vec\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embed_dim, num_heads, drop_rate, context_length):\n",
    "    super().__init__()\n",
    "    assert embed_dim % num_heads == 0\n",
    "\n",
    "    # 헤드당 차원 계산\n",
    "    atten_dim = embed_dim // num_heads\n",
    "\n",
    "    # 여러 개의 SelfAttention을 사용\n",
    "    self.attentions = nn.ModuleList([SelfAttention(embed_dim, atten_dim, drop_rate, context_length) for _ in range(num_heads)])\n",
    "\n",
    "    # 최종 출력 프로젝션\n",
    "    self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    head_outputs = []\n",
    "\n",
    "    # 헤드 병합(마지막 차원(atten_dim 기준))\n",
    "    for attention in self.attentions:\n",
    "      head_output = attention(x)\n",
    "      head_outputs.append(head_output)\n",
    "    concatenated_heads = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "    # 최종 프로젝션\n",
    "    output = self.fc(concatenated_heads)\n",
    "\n",
    "    return output\n",
    "  \n",
    "\n",
    "# LayerNorm class 정의\n",
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "\n",
    "    # γ는 처음에는 곱해도 그대로 나와야 하기에 1\n",
    "    # β는 처음에는 더해도 그대로 나와야 하기에 0\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "  # norm_x = (x - μ) / sqrt(σ² + ε)\n",
    "  # 최종 출력 = γ * norm_x + β\n",
    "  def forward(self, x):\n",
    "\n",
    "    # keepdim=True => shape: (batch, seq_len, 1) => broadcasting이 가능하게 유지\n",
    "    mean = x.mean(dim=-1, keepdim=True)  \n",
    "    # unbiased=False => 전체데이터의 특성을 그대로 반영 => n-1 이 아니라 n 으로 나눠줌\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "    output = self.scale * norm_x + self.shift\n",
    "    return output\n",
    "  \n",
    "\n",
    "# GELU class 정의\n",
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  # GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))\n",
    "  def forward(self, x):\n",
    "    output = 0.5 * x * (1 + torch.tanh(\n",
    "      torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
    "      x + 0.044715 * torch.pow(x, 3)))\n",
    "    return output\n",
    "  \n",
    "\n",
    "# FeedForward class 정의\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(emb_dim, 4 * emb_dim),\n",
    "      GELU(),\n",
    "      nn.Linear(4 * emb_dim, emb_dim)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.layers(x)\n",
    "    return output\n",
    "  \n",
    "\n",
    "# TransformerBlock class 정의\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, emb_dim, num_heads, drop_rate, context_length):\n",
    "    super().__init__()\n",
    "    self.att = MultiHeadAttention(emb_dim, num_heads, drop_rate, context_length)\n",
    "    self.ff = FeedForward(emb_dim)\n",
    "    self.norm1 = LayerNorm(emb_dim)\n",
    "    self.norm2 = LayerNorm(emb_dim)\n",
    "    self.drop_shortcut = nn.Dropout(drop_rate)\n",
    "\n",
    "  # Pre-Norm 구조\n",
    "  # x → LayerNorm -> Attention -> Dropout -> Residual\n",
    "  # -> LayerNorm -> FeedForword -> Dropout -> Residual\n",
    "  def forward(self, x):\n",
    "    short_cut = x\n",
    "    x = self.norm1(x)\n",
    "    x = self.att(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + short_cut\n",
    "\n",
    "    short_cut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.ff(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    output = x + short_cut\n",
    "\n",
    "    return output\n",
    "  \n",
    "\n",
    "# GPTModel class 정의\n",
    "class GPTModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
    "    self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n",
    "    self.drop_emb = nn.Dropout(DROP_RATE)\n",
    "    self.trf_blocks = nn.Sequential(*[\n",
    "        TransformerBlock(EMB_DIM, NUM_HEADS, DROP_RATE, CONTEXT_LENGTH) \n",
    "        for _ in range(NUM_LAYERS)\n",
    "    ])\n",
    "\n",
    "    self.final_norm = LayerNorm(EMB_DIM)\n",
    "    self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    _, seq_len = in_idx.shape\n",
    "    print(1)\n",
    "    # GPT 모델 전체 구현\n",
    "    # TokenEmbedding + PositionalEmbedding -> Dropout -> TransformerBlock × L\n",
    "    # -> inal LayerNorm → Linear Projection (Out Head) → Logits 반환\n",
    "    tok_embeds = self.tok_emb(in_idx)\n",
    "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "    x = tok_embeds + pos_embeds\n",
    "    x = self.drop_emb(x)\n",
    "    x = self.trf_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    # 훈련 단계\n",
    "    for input_batch, target_batch in train_loader:\n",
    "        print(1)\n",
    "        \n",
    "        # 이전 gradient를 초기화(정확한 학습을 위해)\n",
    "        optimizer.zero_grad()\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "        logits = model(input_batch)\n",
    "\n",
    "        # [batch_size * seq_len, vocab_size] 랑 [batch_size * seq_len] 비교 => loss(scalar)\n",
    "        loss = loss_fn(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "    # 검증 단계\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_batch, target_batch in val_loader:\n",
    "                input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "                logits = model(input_batch)\n",
    "                loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        epoch_val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
