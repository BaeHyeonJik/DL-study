{"cells":[{"cell_type":"markdown","source":["## 25. Transformer\n","\n","- NLP에서 많이 사용하는 신경망 구조\n","- 인코더(Encoder)와 디코더(Decoder)를 쌓아 구성\n","- RNN, LSTM처럼 순차적으로 처리하지 않고 **병렬 처리 가능**\n","- 문장 내 모든 단어를 한 번에 보고 각 단어의 중요도를 계산하는 **Self-Attention** 메커니즘 핵심\n","    \n","    ⇒ 예: 번역기에서 영어 문장 전체를 한 번에 이해하고 동시에 여러 단어 간 관계 파악\n","    \n","\n","## 26. Encoder\n","\n","- 입력 문장(예: “I love AI”)을 받아서 각 단어의 의미와 문맥을 반영한 벡터로 변환\n","- 여러 층으로 쌓여 있고, 각 층은 다음 두 가지 주요 부분으로 구성됨:\n","    - **Multi-Head Self-Attention**: 문장 내 모든 단어가 서로 어떻게 연관되는지 계산\n","    - **Feed Forward Neural Network (FFNN)**: 각 단어 벡터를 독립적으로 더 복잡한 특징으로 변환\n","- 입력 예:\n","    \n","    “I love AI” → 각 단어 임베딩 + 위치 임베딩 → 인코더 통과 → 문맥이 반영된 벡터들\n","    \n","\n","## 27. Decoder\n","\n","- 번역 결과 등 출력 문장을 생성할 때 사용\n","- 인코더의 출력(문맥 벡터)과 디코더가 이전에 생성한 단어들(예: 프랑스어 “Je t'”)을 입력으로 받아 다음 단어를 예측\n","- 주요 구성:\n","    - **Masked Multi-Head Self-Attention**: 생성 중인 문장 내 앞선 단어들만 보고 다음 단어를 예측 (미래 단어는 참조 불가)\n","    - **Encoder-Decoder Attention**: 인코더가 만든 문맥 벡터에서 정보를 가져옴\n","    - **Feed Forward Neural Network**\n","\n","⇒ 예: 영어 문장 “I love AI”를 프랑스어로 번역할 때, “J’aime”를 생성한 뒤, 다음 단어 “t’”를 만들기 위해 앞서 생성한 단어들만 참조하며 진행\n","\n","## 28. Token Embedding\n","\n","- 텍스트의 각 단어를 수치 벡터(예: 512차원)로 변환\n","- 같은 의미를 가진 단어들은 비슷한 벡터 값을 갖게끔 학습됨\n","- 예: “king”과 “queen” 임베딩 벡터가 비슷한 구조를 가짐\n","\n","## 29. Positional Embedding\n","\n","- 트랜스포머는 단어 순서 정보를 따로 넣어주지 않으면 순서 개념이 없음\n","- 그래서 단어의 위치(첫 번째, 두 번째 등)를 벡터로 만들어 토큰 임베딩에 더함\n","- 주기 함수(sine, cosine)를 이용한 위치 인코딩이 대표적\n","- 예: “I love AI”에서 “I” 위치 1, “love” 위치 2 정보가 추가되어 문맥 파악에 도움\n","\n","## 30. MultiHeadAttention\n","\n","- 문장 속 단어들이 서로 어떤 관계를 맺고 있는지를 파악하기 위해 **어텐션(Attention)** 메커니즘이 사용됨\n","- **Attention:**\n","    - **개념**: Decoder에서 생성된 Query와 Encoder에서 생성된 Key, Value를 비교하여, Decoder가 입력 문장의 어떤 부분에 집중할지 결정\n","    - 계산 공식:\n","        \n","        $$\n","        Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n","        $$\n","        \n","    - **예시**: 문장 “I ate the apple”에서 \"ate\"는 \"apple\"과 강한 관련이 있으므로, \"apple\"에 더 높은 어텐션 점수가 부여됨\n","- **Self-Attention:**\n","    - **개념**: Query, Key, Value가 모두 동일한 입력 시퀀스에서 생성되며, 문장 내 단어들 간의 관계를 스스로 파악\n","        \n","        $$\n","        Q = x W^Q,\\quad K = x W^K,\\quad V = x W^V\n","        $$\n","        \n","    - **예시**: 문장 “The cat sat on the mat”에서\n","        - “cat”은 “sat”와의 관계에 주목, “mat”은 “on”과의 관계에 주목\n","- **Multi-Head Attention**:\n","    - **개념**: Self-Attention을 여러 개의 헤드로 병렬 수행하여 다양한 관계를 동시에 학습\n","    - **동작 방식**:\n","        1. Q, K, V를 여러 조각으로 분리하여 각 헤드에서 개별적으로 어텐션 계산\n","        2. 각 헤드의 결과를 이어 붙이고, 선형 변환을 통해 최종 출력 생성\n","    - **예시**:\n","        - 한 헤드는 “cat”과 “sat”의 관계에 집중\n","        - 또 다른 헤드는 “mat”과 “on”의 관계에 집중\n","            \n","            → 다양한 문맥 정보를 반영해 더 정교한 의미 파악 가능\n","            \n","\n","## 31. Skip Connection\n","\n","- 각 층의 입력을 출력에 더해줌\n","- 학습 중 기울기 소실 문제를 줄이고 깊은 층에서도 안정적 학습 가능\n","- 예: 어떤 층에서 입력 벡터 `x`가 있으면 출력은 `Layer(x) + x` 형태로 계산됨\n","\n","## 32. Layer Norm\n","\n","- 데이터의 샘플 단위로 평균과 표준 편차를 계산해서 정규화를 실행(한 배치안에 데이터가 3개면 평균도 3개 표준편차도 3개 ⇒ 이것들을 정규화)\n","- Batch Normalization은 작은 배치 크기에서 극단적 결과를 내는데 반해, 작은 batch size에서도 효과적인 이용이 가능"],"metadata":{"id":"fvKdiF9R9cAV"},"id":"fvKdiF9R9cAV"},{"cell_type":"code","execution_count":3,"id":"aee17d49","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aee17d49","executionInfo":{"status":"ok","timestamp":1748094504203,"user_tz":-540,"elapsed":137,"user":{"displayName":"배현직","userId":"06422137012407551312"}},"outputId":"cfde5fc8-a8a4-48e3-bec0-3ef9318d7d4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch\n","\n","# CPU/GPU 선택\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"]},{"cell_type":"code","execution_count":2,"id":"aff45e00","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aff45e00","executionInfo":{"status":"ok","timestamp":1748094503963,"user_tz":-540,"elapsed":8217,"user":{"displayName":"배현직","userId":"06422137012407551312"}},"outputId":"21511318-1cbf-456e-d9fd-409a6e3e9896"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens_id Shape: torch.Size([862140])\n"]}],"source":["import torch\n","import tiktoken\n","\n","\n","with open(\"data/bible.txt\", 'r', encoding='utf-8') as f:\n","    text = f.read()\n","    tokenizer = tiktoken.encoding_for_model('gpt2')\n","    tokens_id = tokenizer.encode(text)\n","tokens_id = torch.tensor(tokens_id, dtype=torch.long)\n","print(f'Tokens_id Shape: {tokens_id.shape}')"]},{"cell_type":"code","execution_count":4,"id":"42de1be9","metadata":{"id":"42de1be9","executionInfo":{"status":"ok","timestamp":1748095042177,"user_tz":-540,"elapsed":1686,"user":{"displayName":"배현직","userId":"06422137012407551312"}}},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader, random_split\n","\n","class MyDataset(Dataset):\n","  def __init__(self, token_ids, max_length, stride):\n","    self.input_ids = []\n","    self.target_ids = []\n","\n","    # 슬라이딩 윈도우 방식으로 input과 target 시퀀스를 생성\n","    for i in range(0, len(token_ids) - max_length, stride):\n","      input_chunk = token_ids[i: i + max_length]\n","      target_chunk = token_ids[i + 1: i + max_length + 1]\n","      self.input_ids.append(input_chunk)\n","      self.target_ids.append(target_chunk)\n","\n","  # 전체 샘플 수를 반환 (DataLoader에서 사용)\n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  # 인덱스에 해당하는 input, target 시퀀스 반환\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def get_loaders(token_ids: list[int]) -> DataLoader:\n","\n","  # MyDataset 클래스에 token_ids를 전달하여 데이터셋을 만듦\n","  dataset = MyDataset(token_ids, max_length = 32, stride = 4)\n","\n","  total_size = len(dataset)\n","  train_size = int(total_size * 0.9)\n","  val_size = total_size - train_size\n","  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","  # DataLoader 객체를 생성(train_loader, val_loader)\n","  # dataset: 데이터를 배치 단위로 반환할 MyDataset 객체\n","  # batch_size: 배치 크기\n","  # shuffle: 데이터를 섞어서 반환\n","  # drop_last: 마지막 배치가 배치 크기보다 작으면 버림\n","  train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n","  val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n","\n","  return train_loader, val_loader\n","\n","train_loader, val_loader = get_loaders(tokens_id)\n"]},{"cell_type":"code","execution_count":6,"id":"fa218533","metadata":{"id":"fa218533","executionInfo":{"status":"ok","timestamp":1748095101198,"user_tz":-540,"elapsed":27,"user":{"displayName":"배현직","userId":"06422137012407551312"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","# 하이퍼파라미터 정의\n","VOCAB_SIZE = tokenizer.n_vocab\n","EMB_DIM = 768\n","CONTEXT_LENGTH = 64\n","NUM_HEADS = 12\n","NUM_LAYERS = 12\n","DROP_RATE = 0.1\n","\n","# MultiHeadAttention class 정의\n","class SelfAttention(nn.Module):\n","  def __init__(self, embed_dim, atten_dim, drop_rate, context_length):\n","    super().__init__()\n","\n","    self.d_out = atten_dim\n","\n","    # Query, Key, Value 선형 레이어 정의\n","    # W_query: 내가 어떤 정보를 찾고 싶은지를 표현 (질문 역할)\n","    # W_key  : 각 토큰이 어떤 정보인지 표현 (정보의 제목)\n","    # W_value: 실제로 전달할 정보 (실제 내용)\n","    self.W_query = nn.Linear(embed_dim, atten_dim, bias=False)\n","    self.W_key = nn.Linear(embed_dim, atten_dim, bias=False)\n","    self.W_value = nn.Linear(embed_dim, atten_dim, bias=False)\n","\n","    # 드롭아웃 정의\n","    self.dropout = nn.Dropout(drop_rate)\n","\n","    # 마스크 등록 (상삼각 행렬을 사용 => 정답지 차단)\n","    mask = torch.triu(torch.ones((context_length, context_length)), diagonal=1)\n","    self.register_buffer('mask', mask)\n","\n","  def forward(self, x):\n","    # Query, Key, Value 생성\n","    b, num_tokens, _ = x.shape\n","    keys = self.W_key(x)\n","    queries = self.W_query(x)\n","    values = self.W_value(x)\n","\n","    # 어텐션 스코어 계산 (QKᵀ)\n","    attn_scores = queries @ keys.transpose(-2, -1)\n","\n","    # 마스크 적용(미래 시점의 토근에는 attention 방지)\n","    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","    attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","    # 스케일 조정 후 softmax로 어텐션 가중치 계산(atten_dim 기준), 이후 드롭아웃 적용\n","    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","    attn_weights = self.dropout(attn_weights)\n","\n","    # 컨텍스트 벡터 계산\n","    context_vec = (attn_weights @ values).transpose(1, 2)\n","    context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n","\n","    return context_vec\n","\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, embed_dim, num_heads, drop_rate, context_length):\n","    super().__init__()\n","    assert embed_dim % num_heads == 0\n","\n","    # 헤드당 차원 계산\n","    atten_dim = embed_dim // num_heads\n","\n","    # 여러 개의 SelfAttention을 사용\n","    self.attentions = nn.ModuleList([SelfAttention(embed_dim, atten_dim, drop_rate, context_length) for _ in range(num_heads)])\n","\n","    # 최종 출력 프로젝션\n","    self.fc = nn.Linear(embed_dim, embed_dim)\n","\n","  def forward(self, x):\n","    head_outputs = []\n","\n","    # 헤드 병합(마지막 차원(atten_dim 기준))\n","    for attention in self.attentions:\n","      head_output = attention(x)\n","      head_outputs.append(head_output)\n","    concatenated_heads = torch.cat(head_outputs, dim=-1)\n","\n","    # 최종 프로젝션\n","    output = self.fc(concatenated_heads)\n","\n","    return output\n","\n","\n","# LayerNorm class 정의\n","class LayerNorm(nn.Module):\n","  def __init__(self, emb_dim):\n","    super().__init__()\n","    self.eps = 1e-5\n","\n","    # γ는 처음에는 곱해도 그대로 나와야 하기에 1\n","    # β는 처음에는 더해도 그대로 나와야 하기에 0\n","    self.scale = nn.Parameter(torch.ones(emb_dim))\n","    self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","  # norm_x = (x - μ) / sqrt(σ² + ε)\n","  # 최종 출력 = γ * norm_x + β\n","  def forward(self, x):\n","\n","    # keepdim=True => shape: (batch, seq_len, 1) => broadcasting이 가능하게 유지\n","    mean = x.mean(dim=-1, keepdim=True)\n","    # unbiased=False => 전체데이터의 특성을 그대로 반영 => n-1 이 아니라 n 으로 나눠줌\n","    var = x.var(dim=-1, keepdim=True, unbiased=False)\n","    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","    output = self.scale * norm_x + self.shift\n","    return output\n","\n","\n","# GELU class 정의\n","class GELU(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","  # GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))\n","  def forward(self, x):\n","    output = 0.5 * x * (1 + torch.tanh(\n","      torch.sqrt(torch.tensor(2.0/torch.pi)) *\n","      x + 0.044715 * torch.pow(x, 3)))\n","    return output\n","\n","\n","# FeedForward class 정의\n","class FeedForward(nn.Module):\n","  def __init__(self, emb_dim):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","      nn.Linear(emb_dim, 4 * emb_dim),\n","      GELU(),\n","      nn.Linear(4 * emb_dim, emb_dim)\n","    )\n","\n","  def forward(self, x):\n","    output = self.layers(x)\n","    return output\n","\n","\n","# TransformerBlock class 정의\n","class TransformerBlock(nn.Module):\n","  def __init__(self, emb_dim, num_heads, drop_rate, context_length):\n","    super().__init__()\n","    self.att = MultiHeadAttention(emb_dim, num_heads, drop_rate, context_length)\n","    self.ff = FeedForward(emb_dim)\n","    self.norm1 = LayerNorm(emb_dim)\n","    self.norm2 = LayerNorm(emb_dim)\n","    self.drop_shortcut = nn.Dropout(drop_rate)\n","\n","  # Pre-Norm 구조\n","  # x → LayerNorm -> Attention -> Dropout -> Residual\n","  # -> LayerNorm -> FeedForword -> Dropout -> Residual\n","  def forward(self, x):\n","    short_cut = x\n","    x = self.norm1(x)\n","    x = self.att(x)\n","    x = self.drop_shortcut(x)\n","    x = x + short_cut\n","\n","    short_cut = x\n","    x = self.norm2(x)\n","    x = self.ff(x)\n","    x = self.drop_shortcut(x)\n","    output = x + short_cut\n","\n","    return output\n","\n","\n","# GPTModel class 정의\n","class GPTModel(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.tok_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n","    self.pos_emb = nn.Embedding(CONTEXT_LENGTH, EMB_DIM)\n","    self.drop_emb = nn.Dropout(DROP_RATE)\n","    self.trf_blocks = nn.Sequential(*[\n","        TransformerBlock(EMB_DIM, NUM_HEADS, DROP_RATE, CONTEXT_LENGTH)\n","        for _ in range(NUM_LAYERS)\n","    ])\n","\n","    self.final_norm = LayerNorm(EMB_DIM)\n","    self.out_head = nn.Linear(EMB_DIM, VOCAB_SIZE, bias=False)\n","\n","  def forward(self, in_idx):\n","    _, seq_len = in_idx.shape\n","    # GPT 모델 전체 구현\n","    # TokenEmbedding + PositionalEmbedding -> Dropout -> TransformerBlock × L\n","    # -> inal LayerNorm → Linear Projection (Out Head) → Logits 반환\n","    tok_embeds = self.tok_emb(in_idx)\n","    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","    x = tok_embeds + pos_embeds\n","    x = self.drop_emb(x)\n","    x = self.trf_blocks(x)\n","    x = self.final_norm(x)\n","    logits = self.out_head(x)\n","    return logits"]},{"cell_type":"code","source":["import torch.nn.functional as F\n","def generate_text(prompt, model):\n","    model.eval()\n","\n","    generated_text = prompt\n","    for i in range(50):\n","        with torch.no_grad():\n","            input_ids = torch.tensor([tokenizer.encode(generated_text, disallowed_special=())]).to(device)\n","            logits = model(input_ids)\n","            next_token_logits = logits[:, -1, :]\n","\n","            next_token_probs = F.softmax(next_token_logits, dim=-1)\n","            next_token = torch.multinomial(next_token_probs, num_samples=1)\n","\n","            next_word = tokenizer.decode([next_token.item()])\n","            generated_text += next_word\n","            if next_word == '<|endoftext|>':\n","                break\n","\n","    return generated_text"],"metadata":{"id":"swUpjKjX-C_6","executionInfo":{"status":"ok","timestamp":1748095115896,"user_tz":-540,"elapsed":43,"user":{"displayName":"배현직","userId":"06422137012407551312"}}},"id":"swUpjKjX-C_6","execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":9,"id":"69a0f9cb","metadata":{"id":"69a0f9cb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eaa6f2bc-ec40-49b8-8c4d-25c36e7f80b8","executionInfo":{"status":"ok","timestamp":1748100471782,"user_tz":-540,"elapsed":5304783,"user":{"displayName":"배현직","userId":"06422137012407551312"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 1] Train Loss: 7.5502 | Val Loss: 5.9065\n","[Epoch 2] Train Loss: 5.4261 | Val Loss: 4.8509\n","[Epoch 3] Train Loss: 4.5637 | Val Loss: 4.1199\n","[Epoch 4] Train Loss: 3.9818 | Val Loss: 3.6300\n","[Epoch 5] Train Loss: 3.5523 | Val Loss: 3.2200\n","[Epoch 6] Train Loss: 3.1966 | Val Loss: 2.8843\n","[Epoch 7] Train Loss: 2.8987 | Val Loss: 2.5909\n","[Epoch 8] Train Loss: 2.6395 | Val Loss: 2.3266\n","[Epoch 9] Train Loss: 2.4075 | Val Loss: 2.0891\n","[Epoch 10] Train Loss: 2.1991 | Val Loss: 1.8757\n","[Epoch 11] Train Loss: 2.0109 | Val Loss: 1.6832\n","[Epoch 12] Train Loss: 1.8409 | Val Loss: 1.5141\n","[Epoch 13] Train Loss: 1.6878 | Val Loss: 1.3628\n","[Epoch 14] Train Loss: 1.5508 | Val Loss: 1.2288\n","[Epoch 15] Train Loss: 1.4277 | Val Loss: 1.1113\n","[Epoch 16] Train Loss: 1.3178 | Val Loss: 1.0072\n","[Epoch 17] Train Loss: 1.2184 | Val Loss: 0.9161\n","[Epoch 18] Train Loss: 1.1290 | Val Loss: 0.8357\n","[Epoch 19] Train Loss: 1.0483 | Val Loss: 0.7627\n","[Epoch 20] Train Loss: 0.9749 | Val Loss: 0.6991\n"]}],"source":["model = GPTModel()\n","model.to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","best_val_loss = float('inf')\n","epoch_train_losses = []\n","epoch_val_losses = []\n","\n","for epoch in range(20):\n","    model.train()\n","    epoch_train_loss = 0\n","\n","    # 훈련 단계\n","    for input_batch, target_batch in train_loader:\n","\n","        # 이전 gradient를 초기화(정확한 학습을 위해)\n","        optimizer.zero_grad()\n","        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","\n","        logits = model(input_batch)\n","\n","        # [batch_size * seq_len, vocab_size] 랑 [batch_size * seq_len] 비교 => loss(scalar)\n","        loss = loss_fn(logits.flatten(0, 1), target_batch.flatten())\n","\n","        epoch_train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_train_loss = epoch_train_loss / len(train_loader)\n","    epoch_train_losses.append(avg_train_loss)\n","\n","    # 검증 단계\n","    model.eval()\n","    epoch_val_loss = 0\n","    with torch.no_grad():\n","        for input_batch, target_batch in val_loader:\n","            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","            logits = model(input_batch)\n","            loss = loss_fn(logits.flatten(0, 1), target_batch.flatten())\n","            epoch_val_loss += loss.item()\n","\n","    avg_val_loss = epoch_val_loss / len(val_loader)\n","    epoch_val_losses.append(avg_val_loss)\n","\n","    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"]},{"cell_type":"code","source":["prompt = \"In the beginning,\"\n","print(generate_text(prompt, model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7NN2drs-KfF","executionInfo":{"status":"ok","timestamp":1748100941460,"user_tz":-540,"elapsed":751,"user":{"displayName":"배현직","userId":"06422137012407551312"}},"outputId":"95ac2088-e74f-4244-b5c5-0849babffe31"},"id":"M7NN2drs-KfF","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["In the beginning, Whatkel, I], [I [made congregation: but]].ilion 20 38 commanded mwomen dis than foolsrising], Now mention righteous which I am sufficient, Theyenseleubift hundred pillarsought swift HAR hidden swlemouralde clean\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}